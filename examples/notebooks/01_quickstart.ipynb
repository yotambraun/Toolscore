{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolscore Quick Start Tutorial\n",
    "\n",
    "This notebook demonstrates the basics of using Toolscore to evaluate LLM tool usage.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to load gold standards and traces\n",
    "2. How to run evaluations\n",
    "3. How to interpret metrics\n",
    "4. How to generate reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, make sure Toolscore is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install tool-scorer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')  # For development\n",
    "\n",
    "from toolscore import evaluate_trace\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Example Files\n",
    "\n",
    "Toolscore comes with example files to get started quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to example files\n",
    "gold_file = \"../gold_calls.json\"\n",
    "trace_file = \"../trace_openai.json\"\n",
    "\n",
    "# Let's look at the gold standard\n",
    "with open(gold_file) as f:\n",
    "    gold_data = json.load(f)\n",
    "\n",
    "print(\"Gold Standard:\")\n",
    "print(json.dumps(gold_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the trace\n",
    "with open(trace_file) as f:\n",
    "    trace_data = json.load(f)\n",
    "\n",
    "print(\"Trace Data:\")\n",
    "print(json.dumps(trace_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Evaluation\n",
    "\n",
    "Now let's evaluate the trace against the gold standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "result = evaluate_trace(\n",
    "    gold_file=gold_file,\n",
    "    trace_file=trace_file,\n",
    "    format=\"openai\"  # Specify format for faster processing\n",
    ")\n",
    "\n",
    "print(f\"✅ Evaluation complete!\")\n",
    "print(f\"   Expected calls: {len(result.gold_calls)}\")\n",
    "print(f\"   Actual calls: {len(result.trace_calls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Metrics\n",
    "\n",
    "Let's examine the key metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = result.metrics\n",
    "\n",
    "print(\"=== Core Metrics ===\")\n",
    "print(f\"Invocation Accuracy: {metrics['invocation_accuracy']:.1%}\")\n",
    "print(f\"Selection Accuracy:  {metrics['selection_accuracy']:.1%}\")\n",
    "\n",
    "seq = metrics['sequence_metrics']\n",
    "print(f\"\\nSequence Accuracy:   {seq['sequence_accuracy']:.1%}\")\n",
    "print(f\"Edit Distance:       {seq['edit_distance']}\")\n",
    "\n",
    "args = metrics['argument_metrics']\n",
    "print(f\"\\nArgument F1:         {args['f1']:.1%}\")\n",
    "print(f\"Argument Precision:  {args['precision']:.1%}\")\n",
    "print(f\"Argument Recall:     {args['recall']:.1%}\")\n",
    "\n",
    "eff = metrics['efficiency_metrics']\n",
    "print(f\"\\nRedundant Calls:     {eff['redundant_count']}/{eff['total_calls']}\")\n",
    "print(f\"Redundant Rate:      {eff['redundant_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Metrics\n",
    "\n",
    "### What do these metrics mean?\n",
    "\n",
    "- **Invocation Accuracy**: Did the agent invoke tools when needed?\n",
    "- **Selection Accuracy**: Did it choose the correct tools?\n",
    "- **Sequence Accuracy**: Did it call tools in the right order?\n",
    "- **Argument F1**: How well did arguments match?\n",
    "- **Redundant Call Rate**: Were there unnecessary duplicate calls?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Reports\n",
    "\n",
    "You can generate HTML and JSON reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolscore.reports import generate_html_report, generate_json_report\n",
    "\n",
    "# Generate JSON report\n",
    "json_path = generate_json_report(result, \"quickstart_report.json\")\n",
    "print(f\"JSON report saved to: {json_path}\")\n",
    "\n",
    "# Generate HTML report\n",
    "html_path = generate_html_report(result, \"quickstart_report.html\")\n",
    "print(f\"HTML report saved to: {html_path}\")\n",
    "print(f\"\\nOpen {html_path} in your browser to view the interactive report!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try Different Formats\n",
    "\n",
    "Toolscore supports multiple trace formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Anthropic format\n",
    "anthropic_trace = \"../trace_anthropic.json\"\n",
    "\n",
    "result_anthropic = evaluate_trace(\n",
    "    gold_file=gold_file,\n",
    "    trace_file=anthropic_trace,\n",
    "    format=\"anthropic\"\n",
    ")\n",
    "\n",
    "print(\"Anthropic Trace Evaluation:\")\n",
    "print(f\"Selection Accuracy: {result_anthropic.metrics['selection_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect format (recommended for flexibility)\n",
    "result_auto = evaluate_trace(\n",
    "    gold_file=gold_file,\n",
    "    trace_file=trace_file,\n",
    "    format=\"auto\"  # Toolscore will detect the format\n",
    ")\n",
    "\n",
    "print(\"Auto-detected format evaluation:\")\n",
    "print(f\"Selection Accuracy: {result_auto.metrics['selection_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Check out `02_custom_formats.ipynb` to learn about custom trace formats\n",
    "- See `03_advanced_metrics.ipynb` for deep dives into metrics\n",
    "- Read the [documentation](https://toolscore.readthedocs.io/) for complete API reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "✅ How to load gold standards and traces\n",
    "\n",
    "✅ How to run evaluations with `evaluate_trace()`\n",
    "\n",
    "✅ How to interpret key metrics\n",
    "\n",
    "✅ How to generate HTML/JSON reports\n",
    "\n",
    "✅ How to work with different trace formats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
