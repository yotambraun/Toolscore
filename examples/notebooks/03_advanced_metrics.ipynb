{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Metrics Deep Dive\n",
    "\n",
    "This notebook provides an in-depth look at Toolscore's metrics and how to interpret them.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Understanding each metric in detail\n",
    "2. When to use which metric\n",
    "3. How to optimize for specific metrics\n",
    "4. Real-world metric interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')  # For development\n",
    "\n",
    "from toolscore import evaluate_trace\n",
    "from toolscore.adapters.base import ToolCall\n",
    "from toolscore.metrics import (\n",
    "    calculate_invocation_accuracy,\n",
    "    calculate_selection_accuracy,\n",
    "    calculate_edit_distance,\n",
    "    calculate_argument_f1,\n",
    "    calculate_redundant_call_rate\n",
    ")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Invocation Accuracy\n",
    "\n",
    "**Question**: Did the agent invoke tools when needed and refrain when not needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Perfect match\n",
    "gold = [ToolCall(tool=\"search\"), ToolCall(tool=\"summarize\")]\n",
    "trace = [ToolCall(tool=\"search\"), ToolCall(tool=\"summarize\")]\n",
    "\n",
    "accuracy = calculate_invocation_accuracy(gold, trace)\n",
    "print(f\"Perfect match: {accuracy:.1%}\")\n",
    "\n",
    "# Scenario 2: Missing invocations\n",
    "trace_missing = [ToolCall(tool=\"search\")]  # Missing summarize\n",
    "accuracy_missing = calculate_invocation_accuracy(gold, trace_missing)\n",
    "print(f\"Missing tool: {accuracy_missing:.1%}\")\n",
    "\n",
    "# Scenario 3: Extra invocations\n",
    "trace_extra = [ToolCall(tool=\"search\"), ToolCall(tool=\"summarize\"), ToolCall(tool=\"translate\")]\n",
    "accuracy_extra = calculate_invocation_accuracy(gold, trace_extra)\n",
    "print(f\"Extra tool: {accuracy_extra:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Invocation Accuracy\n",
    "\n",
    "- Detecting if agent is **over/under-using** tools\n",
    "- Ensuring agent knows **when** to use tools\n",
    "- Benchmarking different prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selection Accuracy\n",
    "\n",
    "**Question**: Did the agent choose the correct tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: All correct\n",
    "gold = [ToolCall(tool=\"read\"), ToolCall(tool=\"write\")]\n",
    "trace = [ToolCall(tool=\"read\"), ToolCall(tool=\"write\")]\n",
    "accuracy = calculate_selection_accuracy(gold, trace)\n",
    "print(f\"All correct: {accuracy:.1%}\")\n",
    "\n",
    "# Scenario 2: Half wrong\n",
    "trace_half = [ToolCall(tool=\"read\"), ToolCall(tool=\"delete\")]  # Wrong second tool\n",
    "accuracy_half = calculate_selection_accuracy(gold, trace_half)\n",
    "print(f\"Half wrong: {accuracy_half:.1%}\")\n",
    "\n",
    "# Scenario 3: All wrong\n",
    "trace_wrong = [ToolCall(tool=\"copy\"), ToolCall(tool=\"move\")]\n",
    "accuracy_wrong = calculate_selection_accuracy(gold, trace_wrong)\n",
    "print(f\"All wrong: {accuracy_wrong:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Selection Accuracy\n",
    "\n",
    "- Measuring if agent picks **appropriate tools**\n",
    "- Comparing different models' tool understanding\n",
    "- Identifying confusing tool names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Edit Distance\n",
    "\n",
    "**Question**: Did the agent call tools in the right order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Perfect order\n",
    "gold = [ToolCall(tool=\"A\"), ToolCall(tool=\"B\"), ToolCall(tool=\"C\")]\n",
    "trace = [ToolCall(tool=\"A\"), ToolCall(tool=\"B\"), ToolCall(tool=\"C\")]\n",
    "result = calculate_edit_distance(gold, trace)\n",
    "print(f\"Perfect order: distance={result['edit_distance']}, accuracy={result['sequence_accuracy']:.1%}\")\n",
    "\n",
    "# Scenario 2: Swapped order\n",
    "trace_swap = [ToolCall(tool=\"B\"), ToolCall(tool=\"A\"), ToolCall(tool=\"C\")]\n",
    "result_swap = calculate_edit_distance(gold, trace_swap)\n",
    "print(f\"Swapped: distance={result_swap['edit_distance']}, accuracy={result_swap['sequence_accuracy']:.1%}\")\n",
    "\n",
    "# Scenario 3: Missing step\n",
    "trace_missing = [ToolCall(tool=\"A\"), ToolCall(tool=\"C\")]  # Skipped B\n",
    "result_missing = calculate_edit_distance(gold, trace_missing)\n",
    "print(f\"Missing step: distance={result_missing['edit_distance']}, accuracy={result_missing['sequence_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Sequence Metrics\n",
    "\n",
    "- Workflows where **order matters** (e.g., authenticate → query → logout)\n",
    "- Multi-step planning evaluation\n",
    "- Identifying if agent understands dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Argument F1 Score\n",
    "\n",
    "**Question**: How well did arguments match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Perfect match\n",
    "gold = [ToolCall(tool=\"search\", args={\"query\": \"Python\", \"limit\": 10})]\n",
    "trace = [ToolCall(tool=\"search\", args={\"query\": \"Python\", \"limit\": 10})]\n",
    "result = calculate_argument_f1(gold, trace)\n",
    "print(f\"Perfect: P={result['precision']:.1%}, R={result['recall']:.1%}, F1={result['f1']:.1%}\")\n",
    "\n",
    "# Scenario 2: Missing argument\n",
    "trace_missing = [ToolCall(tool=\"search\", args={\"query\": \"Python\"})]  # Missing limit\n",
    "result_missing = calculate_argument_f1(gold, trace_missing)\n",
    "print(f\"Missing arg: P={result_missing['precision']:.1%}, R={result_missing['recall']:.1%}, F1={result_missing['f1']:.1%}\")\n",
    "\n",
    "# Scenario 3: Extra arguments\n",
    "trace_extra = [ToolCall(tool=\"search\", args={\"query\": \"Python\", \"limit\": 10, \"sort\": \"date\"})]\n",
    "result_extra = calculate_argument_f1(gold, trace_extra)\n",
    "print(f\"Extra arg: P={result_extra['precision']:.1%}, R={result_extra['recall']:.1%}, F1={result_extra['f1']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Precision vs Recall\n",
    "\n",
    "- **Precision**: Of arguments provided, how many were correct?\n",
    "- **Recall**: Of required arguments, how many were provided?\n",
    "- **F1**: Balanced measure (harmonic mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Redundant Call Rate\n",
    "\n",
    "**Question**: Were there unnecessary duplicate calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: No redundant calls\n",
    "gold = [ToolCall(tool=\"A\"), ToolCall(tool=\"B\")]\n",
    "trace = [ToolCall(tool=\"A\"), ToolCall(tool=\"B\")]\n",
    "result = calculate_redundant_call_rate(gold, trace)\n",
    "print(f\"No redundancy: {result['redundant_count']}/{result['total_calls']} ({result['redundant_rate']:.1%})\")\n",
    "\n",
    "# Scenario 2: Extra unnecessary calls\n",
    "trace_extra = [ToolCall(tool=\"A\"), ToolCall(tool=\"B\"), ToolCall(tool=\"C\"), ToolCall(tool=\"D\")]\n",
    "result_extra = calculate_redundant_call_rate(gold, trace_extra)\n",
    "print(f\"With redundancy: {result_extra['redundant_count']}/{result_extra['total_calls']} ({result_extra['redundant_rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Redundancy Rate\n",
    "\n",
    "- Optimizing **efficiency** and **cost**\n",
    "- Detecting repetitive behavior\n",
    "- Identifying prompt improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Example: Comparing Models\n",
    "\n",
    "Let's compare hypothetical GPT-4 vs Claude performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example files\n",
    "gold_file = \"../gold_calls.json\"\n",
    "\n",
    "# Evaluate both traces\n",
    "result_openai = evaluate_trace(gold_file, \"../trace_openai.json\", format=\"openai\")\n",
    "result_anthropic = evaluate_trace(gold_file, \"../trace_anthropic.json\", format=\"anthropic\")\n",
    "\n",
    "# Compare metrics\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(\"\\nInvocation Accuracy:\")\n",
    "print(f\"  OpenAI:    {result_openai.metrics['invocation_accuracy']:.1%}\")\n",
    "print(f\"  Anthropic: {result_anthropic.metrics['invocation_accuracy']:.1%}\")\n",
    "\n",
    "print(\"\\nSelection Accuracy:\")\n",
    "print(f\"  OpenAI:    {result_openai.metrics['selection_accuracy']:.1%}\")\n",
    "print(f\"  Anthropic: {result_anthropic.metrics['selection_accuracy']:.1%}\")\n",
    "\n",
    "print(\"\\nSequence Accuracy:\")\n",
    "print(f\"  OpenAI:    {result_openai.metrics['sequence_metrics']['sequence_accuracy']:.1%}\")\n",
    "print(f\"  Anthropic: {result_anthropic.metrics['sequence_metrics']['sequence_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metric Selection Guide\n",
    "\n",
    "| Use Case | Primary Metrics | Secondary Metrics |\n",
    "|----------|----------------|------------------|\n",
    "| Function calling correctness | Selection Accuracy, Argument F1 | Invocation Accuracy |\n",
    "| Multi-step planning | Sequence Edit Distance | Selection Accuracy |\n",
    "| Cost optimization | Redundant Call Rate | Invocation Accuracy |\n",
    "| Model comparison | All metrics | Side-effect validation |\n",
    "| Prompt engineering | Selection Accuracy, Argument F1 | Redundant Call Rate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tips for Improvement\n",
    "\n",
    "### Low Selection Accuracy?\n",
    "- Improve tool descriptions\n",
    "- Reduce number of similar tools\n",
    "- Add examples to tool documentation\n",
    "\n",
    "### Low Argument F1?\n",
    "- Make parameter names clearer\n",
    "- Provide examples in tool schema\n",
    "- Use stricter typing\n",
    "\n",
    "### High Redundancy?\n",
    "- Improve context management\n",
    "- Add explicit stop conditions\n",
    "- Review prompt for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Deep understanding of each metric\n",
    "\n",
    "✅ When to use which metrics\n",
    "\n",
    "✅ How to interpret metric scores\n",
    "\n",
    "✅ How to compare models systematically\n",
    "\n",
    "✅ Practical tips for improvement\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply these metrics to your own agents\n",
    "- Create custom metrics for specific needs\n",
    "- Read the [metrics API documentation](https://toolscore.readthedocs.io/en/latest/api/metrics.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
